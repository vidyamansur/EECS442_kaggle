{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture Load : Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict\n",
    "\n",
    "DATA_TYPE = tf.float32\n",
    "VARIABLE_COUNTER = 0\n",
    "\n",
    "def variable(name, shape, initializer,regularizer=None):\n",
    "    global VARIABLE_COUNTER\n",
    "    with tf.device('/cpu:0'):\n",
    "        VARIABLE_COUNTER += np.prod(np.array(shape))\n",
    "        return tf.get_variable(name, shape, initializer=initializer, regularizer=regularizer, dtype=DATA_TYPE, trainable=True)\n",
    "\n",
    "def conv_layer(input_tensor,name,kernel_size,output_channels,initializer,stride=1,bn=False,training=False,relu=True):\n",
    "    input_channels = input_tensor.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel = variable('weights', [kernel_size, kernel_size, input_channels, output_channels], initializer, regularizer=tf.contrib.layers.l2_regularizer(0.0005))\n",
    "        conv = tf.nn.conv2d(input_tensor, kernel, [1, stride, stride, 1], padding='SAME')\n",
    "        biases = variable('biases', [output_channels], tf.constant_initializer(0.0))\n",
    "        conv_layer = tf.nn.bias_add(conv, biases)\n",
    "        if bn:\n",
    "            conv_layer = batch_norm_layer(conv_layer,scope,training)\n",
    "        if relu:\n",
    "            conv_layer = tf.nn.relu(conv_layer, name=scope.name)\n",
    "    print('Conv layer {0} -> {1}'.format(input_tensor.get_shape().as_list(),conv_layer.get_shape().as_list()))\n",
    "    return conv_layer\n",
    "\n",
    "def residual_block(input_tensor,name,kernel_size,output_channels,initializer,stride=1,bn=True,training=False):\n",
    "    print('')\n",
    "    print('Residual Block')\n",
    "    input_channels = input_tensor.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        conv_output = conv_layer(input_tensor,'conv1',kernel_size,output_channels,initializer,stride=stride,bn=bn,training=training,relu=True)\n",
    "        conv_output = conv_layer(conv_output,'conv2',kernel_size,output_channels,initializer,stride=1,bn=bn,training=training,relu=False)\n",
    "        if stride != 1 or input_channels != output_channels:\n",
    "            old_input_shape = input_tensor.get_shape().as_list()\n",
    "            input_tensor = conv_layer(input_tensor,'projection',stride,output_channels,initializer,stride=stride,bn=False,training=training,relu=False)\n",
    "            print('Projecting input {0} -> {1}'.format(old_input_shape,input_tensor.get_shape().as_list()))\n",
    "        res_output = tf.nn.relu(input_tensor + conv_output,name=scope.name)\n",
    "    print('')\n",
    "    return res_output\n",
    "\n",
    "def deconv_layer(input_tensor,name,kernel_size,output_channels,initializer,stride=1,bn=False,training=False,relu=True):\n",
    "    input_shape = input_tensor.get_shape().as_list()\n",
    "    input_channels = input_shape[-1]\n",
    "    output_shape = list(input_shape)\n",
    "    output_shape[1] *= stride\n",
    "    output_shape[2] *= stride\n",
    "    output_shape[3] = output_channels\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel = variable('weights', [kernel_size, kernel_size, output_channels, input_channels], initializer, regularizer=tf.contrib.layers.l2_regularizer(0.0005))\n",
    "        deconv = tf.nn.conv2d_transpose(input_tensor, kernel, output_shape, [1, stride, stride, 1], padding='SAME')\n",
    "        biases = variable('biases', [output_channels], tf.constant_initializer(0.0))\n",
    "        deconv_layer = tf.nn.bias_add(deconv, biases)\n",
    "        if bn:\n",
    "            deconv_layer = batch_norm_layer(deconv_layer,scope,training)\n",
    "        if relu:\n",
    "            deconv_layer = tf.nn.relu(deconv_layer, name=scope.name)\n",
    "    print('Deconv layer {0} -> {1}'.format(input_tensor.get_shape().as_list(),deconv_layer.get_shape().as_list()))\n",
    "    return deconv_layer\n",
    "\n",
    "def max_pooling(input_tensor,name,factor=2):\n",
    "    pool = tf.nn.max_pool(input_tensor, ksize=[1, factor, factor, 1], strides=[1, factor, factor, 1], padding='SAME', name=name)\n",
    "    print('Pooling layer {0} -> {1}'.format(input_tensor.get_shape().as_list(),pool.get_shape().as_list()))\n",
    "    return pool\n",
    "\n",
    "def fully_connected_layer(input_tensor,name,output_channels,initializer,bn=False,training=False,relu=True):\n",
    "    input_channels = input_tensor.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = variable('weights', [input_channels, output_channels], initializer, regularizer=tf.contrib.layers.l2_regularizer(0.0005))\n",
    "        biases = variable('biases', [output_channels], tf.constant_initializer(0.0))\n",
    "        fc = tf.add(tf.matmul(input_tensor,weights), biases, name=scope.name)\n",
    "        if bn:\n",
    "            fc = batch_norm_layer(fc,scope,training)\n",
    "        if relu:\n",
    "            fc = tf.nn.relu(bias, name=scope.name)\n",
    "    print('Fully connected layer {0} -> {1}'.format(input_tensor.get_shape().as_list(),fc.get_shape().as_list()))\n",
    "    return fc\n",
    "\n",
    "def batch_norm_layer(input_tensor,scope,training):\n",
    "    return tf.contrib.layers.batch_norm(input_tensor,scope=scope,is_training=training,decay=0.99)\n",
    "\n",
    "def dropout_layer(input_tensor,keep_prob,training):\n",
    "    if training:\n",
    "        return tf.nn.dropout(input_tensor,keep_prob)\n",
    "    return input_tensor\n",
    "\n",
    "def concat_layer(input_tensor1,input_tensor2,axis=3):\n",
    "    output = tf.concat(3,[input_tensor1,input_tensor2])\n",
    "    input1_shape = input_tensor1.get_shape().as_list()\n",
    "    input2_shape = input_tensor2.get_shape().as_list()\n",
    "    output_shape = output.get_shape().as_list()\n",
    "    print('Concat layer {0} and {1} -> {2}'.format(input1_shape,input2_shape,output_shape))\n",
    "    return output\n",
    "\n",
    "def flatten(input_tensor,name):\n",
    "    batch_size = input_tensor.get_shape().as_list()[0]\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        flat = tf.reshape(input_tensor, [batch_size,-1])\n",
    "    print('Flatten layer {0} -> {1}'.format(input_tensor.get_shape().as_list(),flat.get_shape().as_list()))\n",
    "    return flat\n",
    "\n",
    "def classification_inference(images,training=True):\n",
    "    print('-'*30)\n",
    "    print('Network Architecture')\n",
    "    print('-'*30)\n",
    "    global VARIABLE_COUNTER\n",
    "    VARIABLE_COUNTER = 0\n",
    "    layer_name_dict = {}\n",
    "    def layer_name(base_name):\n",
    "        if base_name not in layer_name_dict:\n",
    "            layer_name_dict[base_name] = 0\n",
    "        layer_name_dict[base_name] += 1\n",
    "        name = base_name + str(layer_name_dict[base_name])\n",
    "        return name\n",
    "\n",
    "    NUM_CLASS = 3\n",
    "    dropout_keep_prob = 0.5\n",
    "    bn = True\n",
    "    he_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    x = images\n",
    "    for i in range(2):\n",
    "        x = conv_layer(x,layer_name('conv'),3,64,he_initializer,bn=bn,training=training)\n",
    "    x = max_pooling(x,layer_name('pool'))\n",
    "    x = flatten(x,layer_name('flatten'))\n",
    "    x = fully_connected_layer(x,layer_name('fc'),4096,he_initializer,bn=bn,training=training)\n",
    "    x = dropout_layer(x,dropout_keep_prob,training)\n",
    "    x = fully_connected_layer(x,layer_name('fc'),NUM_CLASS,he_initializer,bn=False,training=training)\n",
    "    print('-'*30)\n",
    "    print('Number of variables:{0}'.format(VARIABLE_COUNTER))\n",
    "    print('-'*30)\n",
    "    print('')\n",
    "    return x\n",
    "\n",
    "\n",
    "def unet(images, training=True):\n",
    "    print('-'*30)\n",
    "    print('Network Architecture')\n",
    "    print('-'*30)\n",
    "    global VARIABLE_COUNTER\n",
    "    VARIABLE_COUNTER = 0\n",
    "    layer_name_dict = {}\n",
    "    def layer_name(base_name):\n",
    "        if base_name not in layer_name_dict:\n",
    "            layer_name_dict[base_name] = 0\n",
    "        layer_name_dict[base_name] += 1\n",
    "        name = base_name + str(layer_name_dict[base_name])\n",
    "        return name\n",
    "        \n",
    "        \n",
    "    NUM_CLASS = 3\n",
    "    bn = True\n",
    "    he_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    x = images  \n",
    "    \n",
    "    dw_h_convs = OrderedDict()\n",
    "    up_h_convs = OrderedDict()\n",
    "    \n",
    "    #Build the network\n",
    "    x = conv_layer(x,layer_name('conv'),3,64,he_initializer, bn = bn, training = training)\n",
    "    dw_h_convs[0] = conv_layer(x,layer_name('conv'),3,64,he_initializer, bn = bn, training = training)\n",
    "    x = max_pooling(dw_h_convs[0], 'pool1')\n",
    "   \n",
    "     \n",
    "    dw_h_convs[1] = conv_layer(x,layer_name('conv'),3 ,128, he_initializer, bn = bn, training = training)\n",
    "    dw_h_convs[1] = conv_layer(dw_h_convs[1],layer_name('conv'),3,128, he_initializer, bn = bn, training = training)\n",
    "    dw_h_convs[2] = max_pooling(dw_h_convs[1],'pool2')\n",
    "    \n",
    "    \n",
    "    dw_h_convs[2] = conv_layer(dw_h_convs[2],layer_name('conv'),3,256,he_initializer, bn = bn, training = training)\n",
    "    dw_h_convs[2] = conv_layer(dw_h_convs[2],layer_name('conv'),3,256,he_initializer, bn = bn, training = training)\n",
    "    dw_h_convs[3] = max_pooling(dw_h_convs[2],'pool3')\n",
    "   \n",
    "    \n",
    "    dw_h_convs[3] = conv_layer(dw_h_convs[3],layer_name('conv'),3,512,he_initializer, bn = bn, training = training)\n",
    "    dw_h_convs[3] = conv_layer(dw_h_convs[3],layer_name('conv'),3,512,he_initializer, bn = bn, training = training)\n",
    "    dw_h_convs[4] = max_pooling(dw_h_convs[3],'pool4')\n",
    "    \n",
    "    \n",
    "    \n",
    "    dw_h_convs[4] = conv_layer(dw_h_convs[4],layer_name('conv'),3,1024, he_initializer, bn = bn, training = training) \n",
    "    dw_h_convs[4] = conv_layer(dw_h_convs[4],layer_name('conv'),3,512, he_initializer, bn = bn, training = training) \n",
    "        \n",
    "    \n",
    "    \n",
    "    up_h_convs[0] = tf.image.resize_images(dw_h_convs[4], [ dw_h_convs[4].get_shape().as_list()[1]*2, \n",
    "                                                            dw_h_convs[4].get_shape().as_list()[2]*2] )  \n",
    "\n",
    "    #print('size of up_h_convs[0] = ', up_h_convs[0].get_shape().as_list())\n",
    "             \n",
    "    up_h_convs[0] = tf.concat([up_h_convs[0], dw_h_convs[3] ],3 ) \n",
    "    up_h_convs[0] = conv_layer(up_h_convs[0], layer_name('conv'), 3, 512, he_initializer, bn = bn, training = training)\n",
    "    up_h_convs[0] = conv_layer(up_h_convs[0], layer_name('conv'), 3, 256, he_initializer, bn = bn, training = training)\n",
    "    \n",
    "    up_h_convs[1] = tf.image.resize_images(up_h_convs[0], [ up_h_convs[0].get_shape().as_list()[1]*2, \n",
    "                                                            up_h_convs[0].get_shape().as_list()[2]*2] )  \n",
    "    \n",
    "    #print('size of up_h_convs[1] = ', up_h_convs[1].get_shape().as_list())    \n",
    "    up_h_convs[1] = tf.concat([up_h_convs[1], dw_h_convs[2] ],3 ) \n",
    "    up_h_convs[1] = conv_layer(up_h_convs[1], layer_name('conv'), 3, 256, he_initializer, bn = bn, training = training)\n",
    "    up_h_convs[1] = conv_layer(up_h_convs[1], layer_name('conv'), 3, 128, he_initializer, bn = bn, training = training)\n",
    "    \n",
    "    up_h_convs[2] = tf.image.resize_images(up_h_convs[1], [ up_h_convs[1].get_shape().as_list()[1]*2, \n",
    "                                                            up_h_convs[1].get_shape().as_list()[2]*2] )  \n",
    "\n",
    "    #print('size of up_h_convs[0] = ', up_h_convs[2].get_shape().as_list())        \n",
    "    up_h_convs[2] = tf.concat([up_h_convs[2], dw_h_convs[1] ],3 ) \n",
    "    up_h_convs[2] = conv_layer(up_h_convs[2], layer_name('conv'), 3, 128, he_initializer, bn = bn, training = training)\n",
    "    up_h_convs[2] = conv_layer(up_h_convs[2], layer_name('conv'), 3, 64, he_initializer, bn = bn, training = training)\n",
    "\n",
    "    up_h_convs[3] = tf.image.resize_images(up_h_convs[2], [ up_h_convs[2].get_shape().as_list()[1]*2, \n",
    "                                                            up_h_convs[2].get_shape().as_list()[2]*2] )\n",
    "                                                            \n",
    "    #print('size of up_h_convs[3] = ', up_h_convs[3].get_shape().as_list())                                                            \n",
    "    #print('size of dw_h_convs[2] = ', dw_h_convs[2].get_shape().as_list())\n",
    "    \n",
    "    up_h_convs[3] = tf.concat([up_h_convs[3], dw_h_convs[0] ],3 ) \n",
    "    up_h_convs[3] = conv_layer(up_h_convs[3], layer_name('conv'), 3, 64, he_initializer, bn = bn, training = training)\n",
    "    up_h_convs[3] = conv_layer(up_h_convs[3], layer_name('conv'), 3, 64, he_initializer, bn = bn, training = training)\n",
    "    \n",
    "\n",
    "    #out = conv_layer(up_h_convs[0], layer_name('conv'), 1, 3, he_initializer, bn = False, training = training, relu=False)\n",
    "    out = conv_layer(up_h_convs[3], layer_name('conv'), 1, 3, he_initializer, bn = False, training = training, relu=False)\n",
    "    out_bhwd = out\n",
    "\n",
    "        \n",
    "    \n",
    "    out = tf.reshape(out,[-1, NUM_CLASS])\n",
    "\n",
    "    \n",
    "\n",
    "    print('size of out= ', out.get_shape().as_list())\n",
    "\n",
    "    return out, out_bhwd\n",
    "\n",
    "def residual_inference(images,training=True):\n",
    "    print('-'*30)\n",
    "    print('Network Architecture')\n",
    "    print('-'*30)\n",
    "    global VARIABLE_COUNTER\n",
    "    VARIABLE_COUNTER = 0\n",
    "    layer_name_dict = {}\n",
    "    def layer_name(base_name):\n",
    "        if base_name not in layer_name_dict:\n",
    "            layer_name_dict[base_name] = 0\n",
    "        layer_name_dict[base_name] += 1\n",
    "        name = base_name + str(layer_name_dict[base_name])\n",
    "        return name\n",
    "\n",
    "    NUM_CLASS = 3\n",
    "    dropout_keep_prob = 0.5\n",
    "    bn = True\n",
    "    he_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    x = images\n",
    "\n",
    "    # Build and return the network\n",
    "    for i in range(4):\n",
    "        x = conv_layer(x,layer_name('conv'),3,64,he_initializer,bn=bn,training=training)\n",
    "    x = residual_block(x,layer_name('resblock'),3,64,he_initializer,stride=2,bn=bn,training=training)\n",
    "    for i in range(8):\n",
    "        x = residual_block(x,layer_name('resblock'),3,64,he_initializer,bn=bn,training=training)\n",
    "    x = residual_block(x,layer_name('resblock'),3,128,he_initializer,stride=2,bn=bn,training=training)\n",
    "    for i in range(16):\n",
    "        x = residual_block(x,layer_name('resblock'),3,128,he_initializer,bn=bn,training=training)\n",
    "    x = deconv_layer(x,layer_name('deconv'),3,128,he_initializer,stride=2,bn=bn,training=training)\n",
    "    x = deconv_layer(x,layer_name('deconv'),3,64,he_initializer,stride=2,bn=bn,training=training)\n",
    "    x = deconv_layer(x,layer_name('deconv'),3,NUM_CLASS,he_initializer,bn=False,training=training,relu=False)\n",
    "    print('-'*30)\n",
    "    print('Number of variables:{0}'.format(VARIABLE_COUNTER))\n",
    "    print('-'*30)\n",
    "    print('')\n",
    "    return x\n",
    "\n",
    "def inference(images,depths,training=True):\n",
    "    print('-'*30)\n",
    "    print('Network Architecture')\n",
    "    print('-'*30)\n",
    "    global VARIABLE_COUNTER\n",
    "    VARIABLE_COUNTER = 0\n",
    "    layer_name_dict = {}\n",
    "    def layer_name(base_name):\n",
    "        if base_name not in layer_name_dict:\n",
    "            layer_name_dict[base_name] = 0\n",
    "        layer_name_dict[base_name] += 1\n",
    "        name = base_name + str(layer_name_dict[base_name])\n",
    "        return name\n",
    "\n",
    "    NUM_CLASS = 3\n",
    "    dropout_keep_prob = 0.5\n",
    "    bn = True\n",
    "    he_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    x = images\n",
    "    y = depths\n",
    "\n",
    "    # Build and return the network\n",
    "    # RGB\n",
    "    for i in range(3):\n",
    "        x = conv_layer(x,layer_name('conv'),3,64,he_initializer,bn=bn,training=training)\n",
    "    x = max_pooling(x,layer_name('max_pooling'))\n",
    "    for i in range(3):\n",
    "        x = conv_layer(x,layer_name('conv'),3,128,he_initializer,bn=bn,training=training)\n",
    "    x = max_pooling(x,layer_name('max_pooling'))\n",
    "\n",
    "    # Depth\n",
    "    for i in range(3):\n",
    "        y = conv_layer(y,layer_name('conv'),3,64,he_initializer,bn=bn,training=training)\n",
    "    y = max_pooling(y,layer_name('max_pooling'))\n",
    "    for i in range(3):\n",
    "        y = conv_layer(y,layer_name('conv'),3,128,he_initializer,bn=bn,training=training)\n",
    "    y = max_pooling(y,layer_name('max_pooling'))\n",
    "\n",
    "    # Concat the two\n",
    "    x = concat_layer(x,y)\n",
    "\n",
    "    x = deconv_layer(x,layer_name('deconv'),3,128,he_initializer,stride=2,bn=bn,training=training)\n",
    "    x = deconv_layer(x,layer_name('deconv'),3,64,he_initializer,stride=2,bn=bn,training=training)\n",
    "    x = deconv_layer(x,layer_name('deconv'),3,NUM_CLASS,he_initializer,bn=False,training=training,relu=False)\n",
    "    print('-'*30)\n",
    "    print('Number of variables:{0}'.format(VARIABLE_COUNTER))\n",
    "    print('-'*30)\n",
    "    print('')\n",
    "    return x\n",
    "\n",
    "# def loss(predictions, labels):\n",
    "#     num_classes = predictions.get_shape().as_list()[-1]\n",
    "#     flat_predictions = tf.reshape(predictions, [-1,num_classes])\n",
    "#     flat_labels = tf.cast(tf.reshape(labels, [-1]), tf.int32)\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(flat_predictions, flat_labels, name='cross_entropy_per_example')\n",
    "#     cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "#     weight_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "#     return tf.add(cross_entropy_mean,weight_loss)\n",
    "\n",
    "def loss(pred_normals,ground_truth,mask): # Is the data preprocessed by this stage or should we do it again here ?\n",
    "    \n",
    "    num_images = 240\n",
    "    mean_angle_error = 0\n",
    "    total_pixels = 0\n",
    "    #for i in range(0,num_images):\n",
    "    total_pixels += np.count_nonzero(mask)\n",
    "    mask = mask != 0\n",
    "        \n",
    "    pred_normals = ((pred_normals / 255.0) - 0.5) * 2\n",
    "    ground_truth = ((ground_truth / 255.0) - 0.5) * 2\n",
    "        \n",
    "\n",
    "    a11 = np.sum(pred_normals * pred_normals, axis=0)[mask]\n",
    "    a22 = np.sum(ground_truth * ground_truth, axis=0)[mask]\n",
    "    a12 = np.sum(pred_normals * ground_truth, axis=0)[mask]\n",
    "\n",
    "    cos_dist = a12 / np.sqrt(a11 * a22)\n",
    "    cos_dist[np.isnan(cos_dist)] = -1\n",
    "    cos_dist = np.clip(cos_dist, -1, 1)\n",
    "    angle_error = np.arccos(cos_dist)\n",
    "    mean_angle_error += np.sum(angle_error)\n",
    "\n",
    "    return mean_angle_error / total_pixels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    batch_size = predictions.get_shape().as_list()[0]\n",
    "    arg_max_preds = tf.argmax(predictions, 3)\n",
    "    flat_predictions = tf.reshape(arg_max_preds, [batch_size,-1])\n",
    "    flat_labels = tf.reshape(labels, [batch_size,-1])\n",
    "    correct_prediction = tf.equal(tf.cast(flat_predictions,tf.int32), flat_labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    return accuracy\n",
    "\n",
    "print(\"Architecture Load : Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import matplotlib as mpl\n",
    "\n",
    "import scipy\n",
    "import matplotlib.image as mpimg\n",
    "import PIL\n",
    "\n",
    "num_images = 20000\n",
    "features = np.array([np.array(PIL.Image.open(\"train/color/\"+str(i)+\".png\"))for i in range(num_images)])\n",
    "normals = np.array([np.array(PIL.Image.open(\"train/normal/\"+str(i)+\".png\"))for i in range(num_images)])\n",
    "mask = np.array([np.array(PIL.Image.open(\"train/mask/\"+str(i)+\".png\"))for i in range(num_images)])\n",
    "xflo = features.astype(np.float32)\n",
    "yflo = normals.astype(np.float32)\n",
    "mflo = mask.astype(np.float32)  \n",
    "\n",
    "\n",
    "x = tf.convert_to_tensor(xflo)\n",
    "y = tf.convert_to_tensor(yflo)\n",
    "#mask = tf.convert_to_tensor(mflo)\n",
    "mask = mflo\n",
    "\n",
    "training_iters = 100\n",
    "display_step = 10\n",
    "\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "base_learning_rate = 0.01\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32,shape=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "# batch_size = 128\n",
    "# img_height = 128\n",
    "# img_width = 128\n",
    "# x = tf.placeholder(tf.float32, [batch_size, img_height, img_width, 3])\n",
    "# y = tf.placeholder(tf.int32, batch_size*img_width*img_height)\n",
    "# y_bool = tf.placeholder(tf.int32, batch_size*img_width*img_height)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Network Architecture\n",
      "------------------------------\n",
      "Conv layer [240, 128, 128, 3] -> [240, 128, 128, 64]\n",
      "Conv layer [240, 128, 128, 64] -> [240, 128, 128, 64]\n",
      "Pooling layer [240, 128, 128, 64] -> [240, 64, 64, 64]\n",
      "Conv layer [240, 64, 64, 64] -> [240, 64, 64, 128]\n",
      "Conv layer [240, 64, 64, 128] -> [240, 64, 64, 128]\n",
      "Pooling layer [240, 64, 64, 128] -> [240, 32, 32, 128]\n",
      "Conv layer [240, 32, 32, 128] -> [240, 32, 32, 256]\n",
      "Conv layer [240, 32, 32, 256] -> [240, 32, 32, 256]\n",
      "Pooling layer [240, 32, 32, 256] -> [240, 16, 16, 256]\n",
      "Conv layer [240, 16, 16, 256] -> [240, 16, 16, 512]\n",
      "Conv layer [240, 16, 16, 512] -> [240, 16, 16, 512]\n",
      "Pooling layer [240, 16, 16, 512] -> [240, 8, 8, 512]\n",
      "Conv layer [240, 8, 8, 512] -> [240, 8, 8, 1024]\n",
      "Conv layer [240, 8, 8, 1024] -> [240, 8, 8, 512]\n",
      "Conv layer [240, 16, 16, 1024] -> [240, 16, 16, 512]\n",
      "Conv layer [240, 16, 16, 512] -> [240, 16, 16, 256]\n",
      "Conv layer [240, 32, 32, 512] -> [240, 32, 32, 256]\n",
      "Conv layer [240, 32, 32, 256] -> [240, 32, 32, 128]\n",
      "Conv layer [240, 64, 64, 256] -> [240, 64, 64, 128]\n",
      "Conv layer [240, 64, 64, 128] -> [240, 64, 64, 64]\n",
      "Conv layer [240, 128, 128, 128] -> [240, 128, 128, 64]\n",
      "Conv layer [240, 128, 128, 64] -> [240, 128, 128, 64]\n",
      "Conv layer [240, 128, 128, 64] -> [240, 128, 128, 3]\n",
      "size of out=  [3932160, 3]\n",
      "This has been initialised\n",
      "epoch =  1 batch =  1.0 loss =  [1.7918797e+11] learning rate = 0.009999999776482582\n",
      "(240, 128, 128, 3)\n",
      "epoch =  1.0 batch =  2.0 loss =  [1.7854074e+11] learning rate = 0.009999999776482582\n",
      "(240, 128, 128, 3)\n",
      "epoch =  1.0 batch =  3.0 loss =  [1.7783937e+11] learning rate = 0.009999999776482582\n",
      "(240, 128, 128, 3)\n",
      "epoch =  1.0 batch =  4.0 loss =  [1.775519e+11] learning rate = 0.009999999776482582\n",
      "(240, 128, 128, 3)\n",
      "epoch =  1.0 batch =  5.0 loss =  [1.7640753e+11] learning rate = 0.009999999776482582\n",
      "(240, 128, 128, 3)\n",
      "epoch =  1.0 batch =  6.0 loss =  [1.7567918e+11] learning rate = 0.009999999776482582\n",
      "(240, 128, 128, 3)\n",
      "epoch =  1.0 batch =  7.0 loss =  [1.7485881e+11] learning rate = 0.009999999776482582\n",
      "(240, 128, 128, 3)\n",
      "epoch =  1.0 batch =  8.0 loss =  [1.7708293e+11] learning rate = 0.009999999776482582\n",
      "(240, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    # Construct model\n",
    "    #pred, pred_bhwd = custom_layers_unet.unet(x, training=True)\n",
    "    predX = unet(x, training=True)\n",
    "    #gt = tf.cast(tf.reshape(y,[-1]),tf.int32)\n",
    "    gt = y\n",
    "    # Define loss and optimizer\n",
    "    \n",
    "    #pred= predX.reshape((120,128,128,3))\n",
    "    pred = predX[1]\n",
    "    #loss_map = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=gt)\n",
    "    #loss_map = tf.multiply(loss_map,tf.to_float(tf.not_equal(gt,0)))\n",
    "    loss= tf.nn.l2_loss(pred - gt)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    #cost = tf.reduce_mean(loss_map)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #total steps\n",
    "    #total_steps = 100\n",
    "\n",
    "# Launch the graph\n",
    "    config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    epoch = 1\n",
    "    #config.gpu_options.allow_growth=True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        print('This has been initialised')    \n",
    "        step = 1\n",
    "        \n",
    "        while 1:\n",
    "            #img, label = SUNRGBD_dataset.get_random_shuffle(batch_size)\n",
    "            img = xflo\n",
    "            label = yflo\n",
    "            \n",
    "            #print(img.shape)\n",
    "\n",
    "            #plt.show()\n",
    "            #label = np.reshape(label,[-1])\n",
    "            \n",
    "            decay = np.floor((epoch - 1) / 30)\n",
    "            learningRate = base_learning_rate *  np.power(0.95, decay)\n",
    "            #learning_rate = (1/step) * 0.1\n",
    "            \n",
    "            _,lr = sess.run([optimizer,learning_rate], feed_dict={x: img, y: label, learning_rate:learningRate})\n",
    "            loss= sess.run([cost], feed_dict={x: img,y: label})\n",
    "            #print(lmap.shape)\n",
    "            print('epoch = ', epoch, 'batch = ', step-(np.floor(20000/batch_size))*(epoch-1), 'loss = ', loss, 'learning rate =', lr)\n",
    "            step = step + 1\n",
    "            epoch = np.floor(step*batch_size/20000)+1\n",
    "            tpred, tpred_bhwd = sess.run([predX[0], predX[1]], feed_dict={x: img,y: label})\n",
    "            print(tpred_bhwd.shape)\n",
    "            \n",
    "            #best_labels = np.argmax(tpred_bhwd,axis=3)\n",
    "            #print(best_labels[1])\n",
    "            #print(best_labels.shape)\n",
    "                \n",
    "            #batchImage = tile_images(best_labels,batch_size, rows, cols, 1)\n",
    "            #im.set_data(np.uint8(batchImage));\n",
    "            #print('max = ',img[1].max(),'min= ', img[1].min())\n",
    "            #im.set_clim(vmin=0.0, vmax=255.0)\n",
    "            #fig.show();\n",
    "            pl.pause(0.00001);\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_,lr = sess.run([optimizer,learning_rate], feed_dict={x: img, y: label, learning_rate:learningRate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "        _,lr = sess.run([optimizer,learning_rate], feed_dict={x: img, y: label, learning_rate:learningRate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = x\n",
    "label = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
